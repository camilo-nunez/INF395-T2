{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"http://sct.inf.utfsm.cl/wp-content/uploads/2020/04/logo_di.png\" style=\"width:60%\">\n",
    "    <h1> INF-395/477 Redes Neuronales Artificiales I-2020 </h1>\n",
    "    <h3>  Tarea 2 - Redes Neuronales Convolucionales y Recurrentes  </h3>\n",
    "</center>\n",
    "\n",
    "Nombres: Leonardo Astudillo Villalon  & Camilo Nunez Fernandez\n",
    "\n",
    "Roles: 201573598-0 &201573573-5\n",
    "\n",
    "Correos:\n",
    "leonardo.astudillov@sansano.usm.cl & camilo.nunezf@sansano.usm.cl\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "\n",
    "**Temas**  \n",
    "\n",
    "* Entrenamiento de Redes Neuronales Profundas. \n",
    "* Modelos de Auto-Encoder\n",
    "* Redes Convolucionales y Recurrentes. \n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1 - Tema Libre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tema\n",
    "> Tema 13. No tiene sentido usar una red convolucional para aprendizaje de secuencias, su error será siempre mayor que el de una red recurrente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MC0aIMdBsy8F"
   },
   "source": [
    "## Variables generales y de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P16ZFRDq7WVE"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, LSTM, Conv1D, MaxPooling1D, Input, GlobalMaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"brown\")\n",
    "from nltk.corpus import brown\n",
    "brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6vKm2e8yi5T"
   },
   "outputs": [],
   "source": [
    "! mkdir data_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MiwR3Q4ayjCP"
   },
   "outputs": [],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0uZfsxuyq7N"
   },
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle\n",
    "! touch ~/.kaggle/kaggle.json\n",
    "! echo '{\"username\":\"verack\",\"key\":\"adbc9010cabd6ada149effbea0960f9b\"}' > ~/.kaggle/kaggle.json\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qWnkeWryrRk"
   },
   "outputs": [],
   "source": [
    "! rm -rf data_in/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PI3JZvtcUxNh"
   },
   "source": [
    "Descarga de los vectores de gloVe (100)\n",
    "Link: https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "byziI6vMUwJR",
    "outputId": "af04d4ec-5aa8-46b3-a721-0b6ddbc89423"
   },
   "outputs": [],
   "source": [
    "! mkdir data_in/glove\n",
    "! kaggle datasets download rtatman/glove-global-vectors-for-word-representation -p data_in/glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "ffD4sE1QWQbC",
    "outputId": "a60ceec1-40e1-4421-c0bf-8c8e3e935a73"
   },
   "outputs": [],
   "source": [
    "! unzip data_in/glove/glove-global-vectors-for-word-representation.zip -d data_in/glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yMuiy0W8Gmx-"
   },
   "source": [
    "Metricas Globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CKg-UgjqGp6f",
    "outputId": "a4eb0834-3a36-4b1a-e039-1e5a17e68546"
   },
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YqpuoG4gf4sC"
   },
   "source": [
    "### Comparación de Redes CNN y RNN para el caso del analisis de sentimientos para las reviews de IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UWieasflStK"
   },
   "source": [
    "Link: https://keras.io/api/datasets/imdb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xxNrZG3YV4f6"
   },
   "source": [
    "#### Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tT8IjKqcbn7Y"
   },
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "JvmrWyhrkb6d",
    "outputId": "4c0f611c-e8e0-447b-91f9-ac48447ad785"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "qaXygbjXkcry",
    "outputId": "7e1abdb8-661c-4225-a3cd-d147a5003a8d"
   },
   "outputs": [],
   "source": [
    "#Calculo del promedio\n",
    "suma = 0\n",
    "for i in range(25000):\n",
    "  suma = len(x_train[i]) + suma\n",
    "print(\"El promedio de palabras para data_train\",suma/25000)\n",
    "\n",
    "suma = 0\n",
    "for i in range(25000):\n",
    "  suma = len(x_test[i]) + suma\n",
    "print(\"El promedio de palabras para data_test\",suma/25000)\n",
    "\n",
    "#average = sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "yaQ1mUdEb1ok",
    "outputId": "cc0b7da7-122a-4d12-c608-cf180a8f1021"
   },
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3G1dQ29RcXKE"
   },
   "source": [
    "modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Cl1CN447b6tQ",
    "outputId": "07e5f133-d2dc-44b7-ff25-e17a505a96b3"
   },
   "outputs": [],
   "source": [
    "print('Build CNN model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(Flatten())\n",
    "#model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "2qF1_nKpe6gM",
    "outputId": "be753f48-750b-4183-e083-dafa45ee8ce3"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "63KolKWLb6xx",
    "outputId": "21a42bae-95ea-423f-ffd3-56fc4f6a8b3f"
   },
   "outputs": [],
   "source": [
    "print('Train CNN model...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Btaf87ekcUAS"
   },
   "source": [
    "modelo RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "lo7BLSNEcSt4",
    "outputId": "363b94ee-0714-4bfb-c770-37e340335185"
   },
   "outputs": [],
   "source": [
    "print('Build RNN model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "#model.add(Conv1D(filters,\n",
    "#                 kernel_size,\n",
    "#                 padding='valid',\n",
    "#                 activation='relu',\n",
    "#                 strides=1))\n",
    "#model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size,dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(lstm_output_size,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "#model.add(Dense(1))\n",
    "#model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "Z8rlUichcfNR",
    "outputId": "73001959-9bbd-4304-883c-bbf5ab4aa2ef"
   },
   "outputs": [],
   "source": [
    "print('Train RNN model...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVFg_sRj3THN"
   },
   "source": [
    "### Comparación de Redes CNN y RNN para el caso de clasificación del tipo de noticias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gei9u4WiVr4v"
   },
   "source": [
    "Descarga del dataset Link: https://www.kaggle.com/wunderbarx/20-newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "gPq1pu2Lc3Bq",
    "outputId": "e35d1211-9003-45ff-f454-f9756c0d1b51"
   },
   "outputs": [],
   "source": [
    "! mkdir data_in/20_news\n",
    "! kaggle datasets download wunderbarx/20-newsgroup -p data_in/20_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xVi-ARaA8oW9",
    "outputId": "70428fda-eecd-4d89-c2f4-a8b04244c1c0"
   },
   "outputs": [],
   "source": [
    "! unzip data_in/20_news/20-newsgroup.zip -d data_in/20_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dfvGNzEt9q9p"
   },
   "source": [
    "Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mASIapDN-qPa"
   },
   "source": [
    "Variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PAhzk0Dg95QV"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = 'data_in/'\n",
    "GLOVE_DIR = os.path.join('data_in/', 'glove')\n",
    "TEXT_DATA_DIR = os.path.join('data_in/20_news', '20_newsgroups')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zhuWtP0-yw6"
   },
   "source": [
    "Configuración del vector de embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "f5TYUraI-tGK",
    "outputId": "4d1a7335-053d-4be5-8bea-4479a75e6980"
   },
   "outputs": [],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYuATcbI_vUA"
   },
   "source": [
    "Se procesa el dataset de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "f0BLHWoN_uhL",
    "outputId": "a7104598-ee26-4861-84bd-f40966a1bad3"
   },
   "outputs": [],
   "source": [
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HULHTPCCdgsT",
    "outputId": "cc48f249-29b3-4f3e-80ac-bb796a391453"
   },
   "outputs": [],
   "source": [
    "suma = 0\n",
    "for texto in texts:\n",
    "  suma = suma + len(texto)\n",
    "promedio = suma/len(texts)\n",
    "print(promedio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cWx6h3nSAG9f"
   },
   "source": [
    "Se vectorizan los inputs de texto a un vector 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "lpAB_TtlAHVN",
    "outputId": "25720905-1c2c-4f95-d5a8-ea2767eedc96"
   },
   "outputs": [],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ih2vcDmkAfe6"
   },
   "source": [
    "Se separan los datos de train y de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NlZAUn2tAe9E"
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n5X4OFT8A3Rq"
   },
   "source": [
    "Se prepara la matriz de embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pGXBmYoNA3j7",
    "outputId": "82236f17-a758-4619-be57-ea892680889a"
   },
   "outputs": [],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vix6YaBTBCdt"
   },
   "source": [
    "Se prepara la capa de embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqLVGumcBC5U"
   },
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8skpdBo7BN7n"
   },
   "source": [
    "Se entrena la red convolucional 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "_Iq2a2m4BOZ7",
    "outputId": "534a1ea7-7352-458b-b9d1-78eda447200a"
   },
   "outputs": [],
   "source": [
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "#x = MaxPooling1D(5)(x)\n",
    "#x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "id": "6Cw9iVq_C2c1",
    "outputId": "7ee423f7-3694-4c76-f094-910025c5f5cd"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "nkIoOZjpCFTg",
    "outputId": "72a72e7e-c6b2-4d92-a416-dcaa7d6ae97a"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "batch_size = 100\n",
    "epochs = 3\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "score, acc = model.evaluate(x_val, y_val, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsR8TmEOGzck"
   },
   "source": [
    "Metricas para la red CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "g4cSTcZVG2AD",
    "outputId": "e89a3baf-323b-4053-a998-7f5ec824433a"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "y_pred = model.predict(x_val)\n",
    "Y_pred = np.argmax(y_pred, 1)\n",
    "Y_test = np.argmax(y_val, 1)\n",
    "mat = confusion_matrix(Y_test, Y_pred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, cbar=False, cmap=plt.cm.Blues)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('True Values');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6tCK9LkHUwK"
   },
   "source": [
    "Tabla de rendimientos para la CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "5XR4acszHfbl",
    "outputId": "dd7af019-fa81-4eaa-8593-d19650d50c40"
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFWubZ_WDkJL"
   },
   "source": [
    "Red LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YgSTJBK2IDbd"
   },
   "outputs": [],
   "source": [
    "# LSTM\n",
    "lstm_output_size = 100\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = LSTM(lstm_output_size)(embedded_sequences)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "-iTQWGegSdWO",
    "outputId": "7f8ec8ca-1d09-4448-d186-b64bc52f30bc"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "batch_size = 100\n",
    "epochs = 3\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "score, acc = model.evaluate(x_val, y_val, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "drfsAlimSk6A",
    "outputId": "319bb665-dc26-44f5-98a9-d4a004c33f53"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "y_pred = model.predict(x_val)\n",
    "Y_pred = np.argmax(y_pred, 1)\n",
    "Y_test = np.argmax(y_val, 1)\n",
    "mat = confusion_matrix(Y_test, Y_pred)\n",
    "sns.heatmap(mat.T, square=True, annot=True, cbar=False, cmap=plt.cm.Blues)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('True Values');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "VstzF6qTS8xZ",
    "outputId": "703dc394-d0eb-42fe-fe1e-560e9f17186f"
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xKzQIfuySwU"
   },
   "source": [
    "## datasets sinteticos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5xR9l1MmDFS"
   },
   "source": [
    "Link: https://keras.io/api/datasets/imdb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZM1YbtQsvFK"
   },
   "source": [
    "Sentiment Analisys with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "teg80FAosvFg"
   },
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_features = 25000\n",
    "parcial_len = 80\n",
    "maxlen = 100\n",
    "difference = maxlen - parcial_len\n",
    "embedding_size = 128\n",
    "\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zO50HVv-378j"
   },
   "source": [
    "Generación de Sustantivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "G4mAmA814A8Z",
    "outputId": "d85ae379-753e-4a82-8a32-abf9f6276180"
   },
   "outputs": [],
   "source": [
    "brown_news_tagged = brown.tagged_words(categories='news')\n",
    "#tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)\n",
    "#tag_fd.keys()\n",
    "lista_sustantivos = []\n",
    "i = 1000 # Se crea una lista de 1000 sustantivos\n",
    "for (word, tag) in brown_news_tagged:\n",
    "  if tag == 'NN' and i > 0:\n",
    "    lista_sustantivos.append(word.lower())\n",
    "    i = i - 1\n",
    "\n",
    "print(lista_sustantivos)\n",
    "print(len(lista_sustantivos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CnkfZ2a03-y9",
    "outputId": "b8093cec-6e4b-41cd-f610-9b975fe64601"
   },
   "outputs": [],
   "source": [
    "word_to_id = imdb.get_word_index()\n",
    "\n",
    "INDEX_FROM=3   # word index offset\n",
    "\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "word_to_id[\"<UNUSED>\"] = 3\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "#print(' '.join(id_to_word[id] for id in train_x[0] ))\n",
    "\n",
    "\n",
    "id_lista_sustantivos = []\n",
    "\n",
    "for word in lista_sustantivos:\n",
    "  try:\n",
    "    id_lista_sustantivos.append(word_to_id[word])\n",
    "  except KeyError:\n",
    "    continue\n",
    "\n",
    "\n",
    "len(id_lista_sustantivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "qX8hDOz-tmTK",
    "outputId": "d3e0153e-ec4c-47dd-e031-36fc93f98a89"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "xslR9rgFtnLG",
    "outputId": "e3d2d837-afe2-4567-c91d-87999fcdc408"
   },
   "outputs": [],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "VAwEHxrsmyd_",
    "outputId": "c75b5b65-106f-45c2-c5dd-7206cfbf6e79"
   },
   "outputs": [],
   "source": [
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "a7eGM2wQgGyt",
    "outputId": "c06f6972-82f1-462f-9bf0-e283d2a7fb20"
   },
   "outputs": [],
   "source": [
    "#Calculo del promedio\n",
    "suma = 0\n",
    "for i in range(25000):\n",
    "  suma = len(x_train[i]) + suma\n",
    "print(\"El promedio de palabras para data_train\",suma/25000)\n",
    "\n",
    "suma = 0\n",
    "for i in range(25000):\n",
    "  suma = len(x_test[i]) + suma\n",
    "print(\"El promedio de palabras para data_test\",suma/25000)\n",
    "\n",
    "#average = sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VUmrtFpHyO3"
   },
   "outputs": [],
   "source": [
    "def agregar_palabras_random(dataset, lista_palabras_random, cant_palabras):\n",
    "  length = len(lista_palabras_random)\n",
    "  for j in range(len(dataset)):\n",
    "    if len(dataset[j]) < 5:\n",
    "        continue\n",
    "    for i in range(cant_palabras-10):\n",
    "      rand_index = randint(2, len(dataset[j])-5)\n",
    "      rand_sust = randint(0, length-1)\n",
    "      dataset[j].insert(rand_index, lista_palabras_random[rand_sust])\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "k5deKkoDGgTL",
    "outputId": "1f2f940b-ace6-4f03-e1c0-e9dcfd5ae8c7"
   },
   "outputs": [],
   "source": [
    "s = \" \"\n",
    "\n",
    "lista1 = []\n",
    "\n",
    "oracion = x_train[0]\n",
    "for i in oracion:\n",
    "  lista1.append(id_to_word[i])\n",
    "\n",
    "print(s.join(lista1))\n",
    "#print(x_train[0])\n",
    "\n",
    "i=0\n",
    "\n",
    "x_train = agregar_palabras_random(x_train, id_lista_sustantivos, difference)\n",
    "x_test = agregar_palabras_random(x_test, id_lista_sustantivos, difference)\n",
    "\n",
    "\n",
    "lista2 = []\n",
    "oracion = x_train[0]\n",
    "for i in oracion:\n",
    "  lista2.append(id_to_word[i])\n",
    "\n",
    "print(s.join(lista2))\n",
    "#print(x_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "jpyq12QIsvFp",
    "outputId": "f62a3bb5-918e-4bde-81cf-f7a9766b2f42"
   },
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pgUxnD_NsvFz"
   },
   "source": [
    "modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "sKsBIvXBsvF2",
    "outputId": "7dfac933-5303-4a5d-9d25-1a2e38bf9ac8"
   },
   "outputs": [],
   "source": [
    "print('Build CNN model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(Flatten())\n",
    "#model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "colab_type": "code",
    "id": "GuFj_LuDsvGC",
    "outputId": "07adea1c-7809-4d38-d25d-a1cf1816be65"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "colab_type": "code",
    "id": "R74tqYQHsvGJ",
    "outputId": "3bceece1-8b48-4cbb-94a8-f0cbde33241a"
   },
   "outputs": [],
   "source": [
    "print('Train CNN model...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_5Jh7S8svGS"
   },
   "source": [
    "modelo RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "MCd_l6v6svGU",
    "outputId": "363b94ee-0714-4bfb-c770-37e340335185"
   },
   "outputs": [],
   "source": [
    "print('Build RNN model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "#model.add(Conv1D(filters,\n",
    "#                 kernel_size,\n",
    "#                 padding='valid',\n",
    "#                 activation='relu',\n",
    "#                 strides=1))\n",
    "#model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(LSTM(lstm_output_size,dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(lstm_output_size,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "#model.add(Dense(1))\n",
    "#model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "78SH0NQ5svGd",
    "outputId": "73001959-9bbd-4304-883c-bbf5ab4aa2ef",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Train RNN model...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2 - Challenge Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import itertools\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "import abc\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.nn.utils as utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = './'\n",
    "\n",
    "train_labels_path = main_path + 'train_labels.csv'\n",
    "train_tree_path = main_path + 'train/'\n",
    "train_source_path = main_path + 'train_source_tweets.txt'\n",
    "\n",
    "test_tree_path = main_path + 'test/'\n",
    "sample_labels_path = main_path + 'sample_submission.csv'\n",
    "test_source_path = main_path + 'test_source_tweets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_errors = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_random(l,n):\n",
    "    return random.sample(l,int(len(l)*(1-n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_list(l,to):\n",
    "    if len(l)>=to:\n",
    "        return remove_random(l,0.5)[:to+1]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(text):\n",
    "    \n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\", \"\")\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \n",
    "    f = lambda t: (1./(max(t, 0) + 1))\n",
    "    \n",
    "    data_child_array = []\n",
    "    time_array = []\n",
    "    \n",
    "    root_tweet_id = ''\n",
    "    \n",
    "    for line in open(path, \"r\", encoding = \"UTF-8\"):\n",
    "        \n",
    "        line = clean_line(line)\n",
    "        line_array = line.split(\"->\")\n",
    "        \n",
    "        parent = line_array[0]\n",
    "        parent_array = parent.split(\",\")\n",
    "        parent_user_id = parent_array[0]\n",
    "        parent_tweet_id = parent_array[1] \n",
    "        parent_time_delay = str(float(parent_array[2]))\n",
    "        \n",
    "        child = line_array[1]\n",
    "        child_array = child.split(\",\")\n",
    "        child_user_id = child_array[0]\n",
    "        child_tweet_id = child_array[1] \n",
    "        child_time_delay = str(float(child_array[2]))\n",
    "        \n",
    "        \n",
    "        if parent_user_id == \"ROOT\":\n",
    "            \n",
    "            source_claim = {\"user_id\" : child_user_id, \"time_delay\" : f(float(child_time_delay)), \"tweet_id\":child_tweet_id}\n",
    "            root_user_id = child_user_id\n",
    "        \n",
    "        if parent_user_id !=root_user_id and parent_user_id != \"ROOT\":\n",
    "            pkg = \"%d:%.10f\"%(int(parent_user_id),f(float(parent_time_delay)))\n",
    "            data_child_array.append(pkg)\n",
    "    \n",
    "    #drop percent data here\n",
    "    #data_child_array = remove_random(data_child_array,0.5)\n",
    "    data_child_array = truncate_list(data_child_array,30)\n",
    "    data_child_array.append(source_claim[\"user_id\"]+\":\"+str(source_claim[\"time_delay\"]))\n",
    "    edges = ' '.join(data_child_array)\n",
    "    tweet_id = int(source_claim['tweet_id'])\n",
    "    \n",
    "    line = \"{}\\t{}\".format(tweet_id,edges)\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_train = glob.glob(os.path.join(train_tree_path, \"*.txt\"))\n",
    "trees_test = glob.glob(os.path.join(test_tree_path, \"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_files_train = []\n",
    "error_files_test = []\n",
    "\n",
    "f = open(\"twitter_graph.txt\", \"w\")\n",
    "\n",
    "for file in trees_train:\n",
    "    try:\n",
    "        f.write(read_data(file)+\"\\n\")\n",
    "    except:\n",
    "        error_files_train.append(file)\n",
    "        print(file)\n",
    "\n",
    "for file in trees_test:\n",
    "    try:\n",
    "        f.write(read_data(file)+\"\\n\")\n",
    "    except:\n",
    "        error_files_test.append(file)\n",
    "        print(file)\n",
    "\n",
    "f.close()\n",
    "\n",
    "error_files_train = [int(i.split('/')[-1][:-4]) for i in error_files_train]\n",
    "error_files_test = [int(i.split('/')[-1][:-4]) for i in error_files_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(train_labels_path)\n",
    "train_source = pd.read_csv(train_source_path,sep='\\t',header=None,names=['id','text'])\n",
    "train_source['label'] = train_labels['label']\n",
    "all_source_train = train_source.set_index('id').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_labels = pd.read_csv(sample_labels_path)\n",
    "test_source = pd.read_csv(test_source_path, sep='\\t',header=None,names=['id','text'])\n",
    "test_source['label'] = sample_labels['label']\n",
    "all_source_test = test_source.set_index('id').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not include_errors:\n",
    "    for _id in error_files_train:\n",
    "        del all_source_train[_id]\n",
    "    for _id in error_files_test:\n",
    "        del all_source_test[_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "keys = list(all_source_train.keys())\n",
    "train_ids, dev_ids = train_test_split(keys, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dim = 300\n",
    "\n",
    "dic = {\n",
    "    'non-rumor': 0,\n",
    "    'false': 1,\n",
    "    'unverified': 2,\n",
    "    'true': 3,\n",
    "    'sample':4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str_cut(string, task=\"\"):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "    words = string.strip().lower().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_symmetric_adjacency_matrix(edges, shape):\n",
    "    def normalize_adj(mx):\n",
    "        \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "        rowsum = np.array(mx.sum(1))\n",
    "        r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "        r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "        r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "        return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "    adj = sp.coo_matrix((edges[:, 2], (edges[:, 0], edges[:, 1])), shape=shape, dtype=np.float32)\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return adj.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tids = []\n",
    "X_uids = []\n",
    "\n",
    "X_train_tid, X_train_content, y_train = [], [], []\n",
    "for _id in train_ids:\n",
    "    text, label = all_source_train[_id]    \n",
    "    X_tids.append(_id)\n",
    "    X_train_tid.append(_id)\n",
    "    X_train_content.append(clean_str_cut(text))\n",
    "    y_train.append(dic[label])\n",
    "    \n",
    "    \n",
    "X_dev_tid, X_dev_content, y_dev = [], [], []\n",
    "for _id in dev_ids:\n",
    "    text, label = all_source_train[_id]\n",
    "    X_tids.append(_id)\n",
    "    X_dev_tid.append(_id)\n",
    "    X_dev_content.append(clean_str_cut(text))\n",
    "    y_dev.append(dic[label]) \n",
    "    \n",
    "\n",
    "X_test_tid, X_test_content, y_test = [], [], []\n",
    "for _id in list(all_source_test.keys()):\n",
    "    text, label = all_source_test[_id]\n",
    "    X_tids.append(_id)\n",
    "    X_test_tid.append(_id)\n",
    "    X_test_content.append(clean_str_cut(text))\n",
    "    y_test.append(dic[label])\n",
    "\n",
    "\n",
    "with open(\"twitter_graph.txt\", 'r', encoding='utf-8') as input:\n",
    "    relation = []\n",
    "    for line in input.readlines():\n",
    "        tmp = line.strip().split()\n",
    "        src = tmp[0]\n",
    "        X_uids.append(src)\n",
    "\n",
    "        for dst_ids_ws in tmp[1:]:\n",
    "            dst, w = dst_ids_ws.split(\":\")\n",
    "            X_uids.append(dst)\n",
    "            relation.append([src, dst, w])    \n",
    "    \n",
    "X_id = list(set(X_tids + X_uids))\n",
    "num_node = len(X_id)\n",
    "print(num_node)\n",
    "X_id_dic = {id:i for i, id in enumerate(X_id)}\n",
    "\n",
    "relation = np.array([[X_id_dic[tup[0]], X_id_dic[tup[1]], tup[2]] for tup in relation])\n",
    "relation = build_symmetric_adjacency_matrix(relation, shape=(num_node, num_node))\n",
    "\n",
    "X_train_tid = np.array([X_id_dic[tid] for tid in X_train_tid])\n",
    "X_dev_tid = np.array([X_id_dic[tid] for tid in X_dev_tid])\n",
    "X_test_tid = np.array([X_id_dic[tid] for tid in X_test_tid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_to_word2vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Load word2vec from Mikolov\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True)\n",
    "    count_missing = 0\n",
    "    for word in vocab:\n",
    "        if model.__contains__(word):\n",
    "            word_vecs[word] = model[word]\n",
    "        else:\n",
    "            #add unknown words by generating random word vectors\n",
    "            count_missing += 1\n",
    "            word_vecs[word] = np.random.uniform(-0.25, 0.25, w2v_dim)\n",
    "\n",
    "    print(str(len(word_vecs) - count_missing)+\" words found in word2vec.\")\n",
    "    print(str(count_missing)+\" words not found, generated by random.\")\n",
    "    return word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_word2vec(sentences, w2v_path='numberbatch-en.txt'):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    vocabulary_inv = []\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv += [x[0] for x in word_counts.most_common() if x[1] >= 2]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "\n",
    "    print(\"embedding_weights generation.......\")\n",
    "    word2vec = vocab_to_word2vec(w2v_path, vocabulary)     #\n",
    "    embedding_weights = build_word_embedding_weights(word2vec, vocabulary_inv)\n",
    "    return vocabulary, embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(X, max_len=50):\n",
    "    X_pad = []\n",
    "    for doc in X:\n",
    "        if len(doc) >= max_len:\n",
    "            doc = doc[:max_len]\n",
    "        else:\n",
    "            doc = [0] * (max_len - len(doc)) + doc\n",
    "        X_pad.append(doc)\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_embedding_weights(word_vecs, vocabulary_inv):\n",
    "    \"\"\"\n",
    "    Get the word embedding matrix, of size(vocabulary_size, word_vector_size)\n",
    "    ith row is the embedding of ith word in vocabulary\n",
    "    \"\"\"\n",
    "    vocab_size = len(vocabulary_inv)\n",
    "    embedding_weights = np.zeros(shape=(vocab_size+1, w2v_dim), dtype='float32')\n",
    "    #initialize the first row\n",
    "    embedding_weights[0] = np.zeros(shape=(w2v_dim,) )\n",
    "\n",
    "    for idx in range(1, vocab_size):\n",
    "        embedding_weights[idx] = word_vecs[vocabulary_inv[idx]]\n",
    "    print(\"Embedding matrix of size \"+str(np.shape(embedding_weights)))\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_data(X, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentencs and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = [[vocabulary[word] for word in sentence if word in vocabulary] for sentence in X]\n",
    "    x = pad_sequence(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_feature_extract(root_path, filename, w2v_path='twitter_w2v.bin'):\n",
    "    \"\"\"X_train_tid, X_train, y_train, \\\n",
    "    X_dev_tid, X_dev, y_dev, \\\n",
    "    X_test_tid, X_test, y_test, relation = read_corpus(root_path, filename)\"\"\"\n",
    "\n",
    "    print(\"text word2vec generation.......\")\n",
    "    vocabulary, word_embeddings = build_vocab_word2vec(X_train + X_dev + X_test, w2v_path=w2v_path)\n",
    "    pickle.dump(vocabulary, open(root_path + \"/vocab.pkl\", 'wb'))\n",
    "    print(\"Vocabulary size: \"+str(len(vocabulary)))\n",
    "\n",
    "    print(\"build input data.......\")\n",
    "    X_train = build_input_data(X_train, vocabulary)\n",
    "    X_dev = build_input_data(X_dev, vocabulary)\n",
    "    X_test = build_input_data(X_test, vocabulary)\n",
    "\n",
    "    pickle.dump([X_train_tid, X_train, y_train, word_embeddings, relation], open(root_path+\"/train.pkl\", 'wb') )\n",
    "    pickle.dump([X_dev_tid, X_dev, y_dev], open(root_path+\"/dev.pkl\", 'wb') )\n",
    "    pickle.dump([X_test_tid, X_test, y_test], open(root_path+\"/test.pkl\", 'wb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tid, X_train, y_train, \\\n",
    "X_dev_tid, X_dev, y_dev, \\\n",
    "X_test_tid, X_test, y_test, relation = X_train_tid, X_train_content, y_train, \\\n",
    "X_dev_tid, X_dev_content, y_dev, \\\n",
    "X_test_tid, X_test_content, y_test, \\\n",
    "relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"text word2vec generation.......\")\n",
    "vocabulary, word_embeddings = build_vocab_word2vec(X_train + X_dev + X_test, w2v_path='twitter_w2v.bin')\n",
    "pickle.dump(vocabulary, open(\"./vocab.pkl\", 'wb'))\n",
    "print(\"Vocabulary size: \"+str(len(vocabulary)))\n",
    "\n",
    "print(\"build input data.......\")\n",
    "X_train = build_input_data(X_train, vocabulary)\n",
    "X_dev = build_input_data(X_dev, vocabulary)\n",
    "X_test = build_input_data(X_test, vocabulary)\n",
    "\n",
    "pickle.dump([X_train_tid, X_train, y_train, word_embeddings, relation], open(\"./train.pkl\", 'wb') )\n",
    "pickle.dump([X_dev_tid, X_dev, y_dev], open(\"./dev.pkl\", 'wb') )\n",
    "pickle.dump([X_test_tid, X_test, y_test], open(\"./test.pkl\", 'wb') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creando la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Special function for only sparse region backpropataion layer.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data)\n",
    "\n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        N = input.size()[0]\n",
    "        edge = torch.LongTensor(adj.nonzero())\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        #e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N, 1)).cuda())\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N, 1)))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "\n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "\n",
    "    def __init__(self, nfeat, uV, adj, hidden=16, nb_heads=8, n_output=300, dropout=0.2, alpha=0.3):\n",
    "        \"\"\"Sparse version of GAT.\"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.uV = uV\n",
    "        self.adj = adj\n",
    "        self.user_tweet_embedding = nn.Embedding(self.uV, 300, padding_idx=0)\n",
    "        init.xavier_uniform_(self.user_tweet_embedding.weight)\n",
    "\n",
    "        self.attentions = nn.ModuleList([SpGraphAttentionLayer(in_features = nfeat,\n",
    "                                                        out_features= hidden,\n",
    "                                                        dropout=dropout,\n",
    "                                                        alpha=alpha,\n",
    "                                                        concat=True) for _ in range(nb_heads)])\n",
    "        \n",
    "        self.out_att = SpGraphAttentionLayer(hidden * nb_heads,\n",
    "                                              n_output,\n",
    "                                             dropout=dropout,\n",
    "                                             alpha=alpha,\n",
    "                                             concat=False)\n",
    "\n",
    "    def forward(self, X_tid):\n",
    "        #X = self.user_tweet_embedding(torch.arange(0, self.uV).long().cuda())\n",
    "        X = self.user_tweet_embedding(torch.arange(0, self.uV).long())\n",
    "        X = self.dropout(X)\n",
    "\n",
    "        X = torch.cat([att(X, self.adj) for att in self.attentions], dim=1)\n",
    "        X = self.dropout(X)\n",
    "\n",
    "        X = F.elu(self.out_att(X, self.adj))\n",
    "        X_ = X[X_tid]\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, d_k=16, d_v=16, n_heads=8, is_layer_norm=False, attn_dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k if d_k is not None else input_size\n",
    "        self.d_v = d_v if d_v is not None else input_size\n",
    "\n",
    "        self.is_layer_norm = is_layer_norm\n",
    "        if is_layer_norm:\n",
    "            self.layer_morm = nn.LayerNorm(normalized_shape=input_size)\n",
    "\n",
    "        self.W_q = nn.Parameter(torch.Tensor(input_size, n_heads * d_k))\n",
    "        self.W_k = nn.Parameter(torch.Tensor(input_size, n_heads * d_k))\n",
    "        self.W_v = nn.Parameter(torch.Tensor(input_size, n_heads * d_v))\n",
    "\n",
    "        self.W_o = nn.Parameter(torch.Tensor(d_v*n_heads, input_size))\n",
    "        self.linear1 = nn.Linear(input_size, input_size)\n",
    "        self.linear2 = nn.Linear(input_size, input_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.__init_weights__()\n",
    "        print(self)\n",
    "\n",
    "    def __init_weights__(self):\n",
    "        init.xavier_normal_(self.W_q)\n",
    "        init.xavier_normal_(self.W_k)\n",
    "        init.xavier_normal_(self.W_v)\n",
    "        init.xavier_normal_(self.W_o)\n",
    "\n",
    "        init.xavier_normal_(self.linear1.weight)\n",
    "        init.xavier_normal_(self.linear2.weight)\n",
    "\n",
    "    def FFN(self, X):\n",
    "        output = self.linear2(F.relu(self.linear1(X)))\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, episilon=1e-6):\n",
    "        '''\n",
    "        :param Q: (*, max_q_words, n_heads, input_size)\n",
    "        :param K: (*, max_k_words, n_heads, input_size)\n",
    "        :param V: (*, max_v_words, n_heads, input_size)\n",
    "        :param episilon:\n",
    "        :return:\n",
    "        '''\n",
    "        temperature = self.d_k ** 0.5\n",
    "        Q_K = torch.einsum(\"bqd,bkd->bqk\", Q, K) / (temperature + episilon)\n",
    "        Q_K_score = F.softmax(Q_K, dim=-1)  # (batch_size, max_q_words, max_k_words)\n",
    "        Q_K_score = self.dropout(Q_K_score)\n",
    "\n",
    "        V_att = Q_K_score.bmm(V)  # (*, max_q_words, input_size)\n",
    "        return V_att\n",
    "\n",
    "\n",
    "    def multi_head_attention(self, Q, K, V):\n",
    "        bsz, q_len, _ = Q.size()\n",
    "        bsz, k_len, _ = K.size()\n",
    "        bsz, v_len, _ = V.size()\n",
    "\n",
    "        Q_ = Q.matmul(self.W_q).view(bsz, q_len, self.n_heads, self.d_k)\n",
    "        K_ = K.matmul(self.W_k).view(bsz, k_len, self.n_heads, self.d_k)\n",
    "        V_ = V.matmul(self.W_v).view(bsz, v_len, self.n_heads, self.d_v)\n",
    "\n",
    "        Q_ = Q_.permute(0, 2, 1, 3).contiguous().view(bsz*self.n_heads, q_len, self.d_k)\n",
    "        K_ = K_.permute(0, 2, 1, 3).contiguous().view(bsz*self.n_heads, q_len, self.d_k)\n",
    "        V_ = V_.permute(0, 2, 1, 3).contiguous().view(bsz*self.n_heads, q_len, self.d_v)\n",
    "\n",
    "        V_att = self.scaled_dot_product_attention(Q_, K_, V_)\n",
    "        V_att = V_att.view(bsz, self.n_heads, q_len, self.d_v)\n",
    "        V_att = V_att.permute(0, 2, 1, 3).contiguous().view(bsz, q_len, self.n_heads*self.d_v)\n",
    "\n",
    "        output = self.dropout(V_att.matmul(self.W_o)) # (batch_size, max_q_words, input_size)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        '''\n",
    "        :param Q: (batch_size, max_q_words, input_size)\n",
    "        :param K: (batch_size, max_k_words, input_size)\n",
    "        :param V: (batch_size, max_v_words, input_size)\n",
    "        :return:  output: (batch_size, max_q_words, input_size)  same size as Q\n",
    "        '''\n",
    "        V_att = self.multi_head_attention(Q, K, V)\n",
    "\n",
    "        if self.is_layer_norm:\n",
    "            X = self.layer_morm(Q + V_att)  # (batch_size, max_r_words, embedding_dim)\n",
    "            output = self.layer_morm(self.FFN(X) + X)\n",
    "        else:\n",
    "            X = Q + V_att\n",
    "            output = self.FFN(X) + X\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.best_acc = 0\n",
    "        self.patience = 0\n",
    "        self.init_clip_max_norm = None\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_train_tid, X_train, y_train,\n",
    "                  X_dev_tid, X_dev, y_dev):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "        batch_size = self.config['batch_size']\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "        X_train_tid = torch.LongTensor(X_train_tid)\n",
    "        X_train = torch.LongTensor(X_train)\n",
    "        y_train = torch.LongTensor(y_train)\n",
    "\n",
    "        dataset = TensorDataset(X_train_tid, X_train, y_train)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        for epoch in range(self.config['epochs']):\n",
    "            print(\"\\nEpoch \", epoch+1, \"/\", self.config['epochs'])\n",
    "            self.train()\n",
    "            avg_loss = 0\n",
    "            avg_acc = 0\n",
    "            for i, data in enumerate(dataloader):\n",
    "                with torch.no_grad():\n",
    "                    #batch_x_tid, batch_x_text, batch_y = (item.cuda(device=self.device) for item in data)\n",
    "                    batch_x_tid, batch_x_text, batch_y = (item.to(self.device) for item in data)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                logit = self.forward(batch_x_tid, batch_x_text)\n",
    "                loss = loss_func(logit, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                corrects = (torch.max(logit, 1)[1].view(batch_y.size()).data == batch_y.data).sum()\n",
    "                accuracy = 100*corrects/len(batch_y)\n",
    "\n",
    "                print('Batch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(i, loss.item(), accuracy, corrects, batch_y.size(0)))\n",
    "                if i > 0 and i % 100 == 0:\n",
    "                    self.evaluate(X_dev, y_dev)\n",
    "                    self.train()\n",
    "\n",
    "                avg_loss += loss.item()\n",
    "                avg_acc += accuracy\n",
    "\n",
    "                if self.init_clip_max_norm is not None:\n",
    "                    utils.clip_grad_norm_(self.parameters(), max_norm=self.init_clip_max_norm)\n",
    "\n",
    "            cnt = y_train.size(0) // batch_size + 1\n",
    "            print(\"Average loss:{:.6f} average acc:{:.6f}%\".format(avg_loss/cnt, avg_acc/cnt))\n",
    "            if epoch >= 10 and self.patience > 3:\n",
    "                print(\"Reload the best model...\")\n",
    "                self.load_state_dict(torch.load(self.config['save_path']))\n",
    "                now_lr = self.adjust_learning_rate(self.optimizer)\n",
    "                print(now_lr)\n",
    "                self.patience = 0\n",
    "\n",
    "            self.evaluate(X_dev_tid, X_dev, y_dev)\n",
    "\n",
    "\n",
    "    def adjust_learning_rate(self, optimizer, decay_rate=.5):\n",
    "        now_lr = 0\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * decay_rate\n",
    "            now_lr = param_group['lr']\n",
    "        return now_lr\n",
    "\n",
    "\n",
    "    def evaluate(self, X_dev_tid, X_dev, y_dev):\n",
    "        y_pred = self.predict(X_dev_tid, X_dev)\n",
    "        acc = accuracy_score(y_dev, y_pred)\n",
    "\n",
    "        if acc > self.best_acc:\n",
    "            self.best_acc = acc\n",
    "            self.patience = 0\n",
    "            torch.save(self.state_dict(), self.config['save_path'])\n",
    "            print(classification_report(y_dev, y_pred, target_names=self.config['target_names'], digits=5))\n",
    "            print(\"Val set acc:\", acc)\n",
    "            print(\"Best val set acc:\", self.best_acc)\n",
    "            print(\"save model!!!\")\n",
    "        else:\n",
    "            self.patience += 1\n",
    "\n",
    "\n",
    "    def predict(self, X_test_tid, X_test):\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "        self.eval()\n",
    "        y_pred = []\n",
    "        #X_test_tid = torch.LongTensor(X_test_tid).cuda()\n",
    "        X_test_tid = torch.LongTensor(X_test_tid)\n",
    "        #X_test = torch.LongTensor(X_test).cuda()\n",
    "        X_test = torch.LongTensor(X_test)\n",
    "\n",
    "        dataset = TensorDataset(X_test_tid, X_test)\n",
    "        dataloader = DataLoader(dataset, batch_size=50)\n",
    "\n",
    "        for i, data in enumerate(dataloader):\n",
    "            with torch.no_grad():\n",
    "                #batch_x_tid, batch_x_text = (item.cuda(device=self.device) for item in data)\n",
    "                batch_x_tid, batch_x_text = (item.to(self.device) for item in data)\n",
    "\n",
    "            logits = self.forward(batch_x_tid, batch_x_text)\n",
    "            predicted = torch.max(logits, dim=1)[1]\n",
    "            y_pred += predicted.data.cpu().numpy().tolist()\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLAN(NeuralNetwork):\n",
    "\n",
    "    def __init__(self, config, adj):\n",
    "        super(GLAN, self).__init__()\n",
    "        self.config = config\n",
    "        self.uV = adj.shape[0]\n",
    "        embedding_weights = config['embedding_weights']\n",
    "        V, D = embedding_weights.shape\n",
    "        maxlen = config['maxlen']\n",
    "        dropout_rate = config['dropout']\n",
    "\n",
    "        self.mh_attention = TransformerBlock(input_size=300)\n",
    "        self.word_embedding = nn.Embedding(V, D, padding_idx=0, _weight=torch.from_numpy(embedding_weights))\n",
    "\n",
    "        self.relation_embedding = GAT(nfeat=300, uV=self.uV, adj=adj)\n",
    "\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(300, 100, kernel_size=K) for K in config['kernel_sizes']])\n",
    "        self.max_poolings = nn.ModuleList([nn.MaxPool1d(kernel_size=maxlen - K + 1) for K in config['kernel_sizes']])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(600, 300)\n",
    "        self.fc2 = nn.Linear(300, config['num_classes'])\n",
    "\n",
    "        self.init_weight()\n",
    "        print(self)\n",
    "\n",
    "    def init_weight(self):\n",
    "        init.xavier_normal_(self.fc1.weight)\n",
    "        init.xavier_normal_(self.fc2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, X_tid, X_text):\n",
    "        X_text = self.word_embedding(X_text) # (N*C, W, D)\n",
    "        X_text = self.mh_attention(X_text, X_text, X_text)\n",
    "        X_text = X_text.permute(0, 2, 1)\n",
    "\n",
    "        rembedding = self.relation_embedding(X_tid)\n",
    "\n",
    "        conv_block = [rembedding]\n",
    "        for _, (Conv, max_pooling) in enumerate(zip(self.convs, self.max_poolings)):\n",
    "            act = self.relu(Conv(X_text))\n",
    "            pool = max_pooling(act)\n",
    "            pool = torch.squeeze(pool)\n",
    "            conv_block.append(pool)\n",
    "        conv_feature = torch.cat(conv_block, dim=1)\n",
    "        features = self.dropout(conv_feature)\n",
    "\n",
    "        a1 = self.relu(self.fc1(features))\n",
    "        d1 = self.dropout(a1)\n",
    "\n",
    "        output = self.fc2(d1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'reg':0,\n",
    "    'batch_size':64,\n",
    "    'nb_filters':100,\n",
    "    'kernel_sizes':[3, 4, 5],\n",
    "    'dropout':0.3,\n",
    "    'maxlen':50,\n",
    "    'epochs':10,\n",
    "    'num_classes':4,\n",
    "    'target_names':['NR', 'FR', 'UR', 'TR']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tid, X_train, y_train, word_embeddings, adj = X_train_tid, X_train, y_train, word_embeddings, relation\n",
    "\n",
    "X_dev_tid, X_dev, y_dev = X_dev_tid, X_dev, y_dev\n",
    "\n",
    "X_test_tid, X_test, y_test = X_test_tid, X_test, y_test\n",
    "\n",
    "config['embedding_weights'] = word_embeddings\n",
    "\n",
    "print(\"#nodes: \", adj.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_suffix = GLAN.__name__.lower().strip(\"text\")\n",
    "config['save_path'] = 'checkpoint/weights.best.twitter.'+ model_suffix\n",
    "\n",
    "mod_nn = GLAN(config, adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mod_nn.fit(X_train_tid, X_train, y_train,X_dev_tid, X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predicction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_nn.load_state_dict(torch.load(config['save_path']))\n",
    "y_pred = mod_nn.predict(X_test_tid, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\n",
    "    0:'non-rumor',\n",
    "    1:'false',\n",
    "    2:'unverified',\n",
    "    3:'true',\n",
    "}\n",
    "sample_pred_label = [dic[i] for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submit = pd.read_csv(sample_labels_path)\n",
    "\n",
    "if not include_errors:\n",
    "    for _id in error_files_test:\n",
    "        indexNames = submit[ submit['id'] == _id ].index\n",
    "        submit.drop(indexNames , inplace=True)\n",
    "\n",
    "        sample_ids = submit['id']\n",
    "        \n",
    "    final_sub = pd.DataFrame ({'label':sample_pred_label+['unverified']*len(error_files_test),'id':list(sample_ids)+error_files_test }, columns = ['label','id'])\n",
    "else:\n",
    "    final_sub = pd.DataFrame ({'label':sample_pred_label,'id':sample_ids }, columns = ['label','id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub.to_csv('./submission_kaggle.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "* https://arxiv.org/abs/1909.04465\n",
    "* https://arxiv.org/abs/1706.03762\n",
    "* https://arxiv.org/abs/1710.10903"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
